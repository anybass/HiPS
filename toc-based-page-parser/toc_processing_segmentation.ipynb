{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97f9ac78-c3bd-480d-aa5a-c257134f1877",
   "metadata": {},
   "source": [
    "# Processing the TOCs of the PDFs for the TOC-based PageParser, the LLM-Refined PageParser and the GT_TOC Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fff2e52-2637-4710-bb31-c7c1964777e4",
   "metadata": {},
   "source": [
    "## Extracting TOC Metadata from the PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd5e2cc-51bb-4c4c-820b-1d81d12c5b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pdfminer.six\n",
    "from enum import Enum, auto\n",
    "from pathlib import Path\n",
    "from typing import Any, Optional\n",
    "from pdfminer.pdfdocument import PDFDocument, PDFNoOutlines\n",
    "from pdfminer.pdfpage import PDFPage, LITERAL_PAGE\n",
    "from pdfminer.pdfparser import PDFParser, PDFSyntaxError\n",
    "from pdfminer.pdftypes import PDFObjRef\n",
    "\n",
    "\n",
    "class PDFRefType(Enum):\n",
    "    \"\"\"PDF reference type.\"\"\"\n",
    "\n",
    "    PDF_OBJ_REF = auto()\n",
    "    DICTIONARY = auto()\n",
    "    LIST = auto()\n",
    "    NAMED_REF = auto()\n",
    "    UNK = auto()  # fallback\n",
    "\n",
    "\n",
    "class RefPageNumberResolver:\n",
    "    \"\"\"PDF Reference to page number resolver.\n",
    "\n",
    "    .. note::\n",
    "\n",
    "       Remote Go-To Actions (see 12.6.4.3 in\n",
    "       `https://www.adobe.com/go/pdfreference/`__)\n",
    "       are out of the scope of this resolver.\n",
    "\n",
    "    Attributes:\n",
    "        document (:obj:`pdfminer.pdfdocument.PDFDocument`):\n",
    "            The document that contains the references.\n",
    "        objid_to_pagenum (:obj:`dict[int, int]`):\n",
    "            Mapping from an object id to the number of the page that contains\n",
    "            that object.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, document: PDFDocument):\n",
    "        self.document = document\n",
    "        # obj_id -> page_number\n",
    "        self.objid_to_pagenum: dict[int, int] = {\n",
    "            page.pageid: page_num\n",
    "            for page_num, page in enumerate(PDFPage.create_pages(document), 1)\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def get_ref_type(cls, ref: Any) -> PDFRefType:\n",
    "        \"\"\"Get the type of a PDF reference.\"\"\"\n",
    "        if isinstance(ref, PDFObjRef):\n",
    "            return PDFRefType.PDF_OBJ_REF\n",
    "        elif isinstance(ref, dict) and \"D\" in ref:\n",
    "            return PDFRefType.DICTIONARY\n",
    "        elif isinstance(ref, list) and any(isinstance(e, PDFObjRef) for e in ref):\n",
    "            return PDFRefType.LIST\n",
    "        elif isinstance(ref, bytes):\n",
    "            return PDFRefType.NAMED_REF\n",
    "        else:\n",
    "            return PDFRefType.UNK\n",
    "\n",
    "    @classmethod\n",
    "    def is_ref_page(cls, ref: Any) -> bool:\n",
    "        \"\"\"Check whether a reference is of type '/Page'.\n",
    "\n",
    "        Args:\n",
    "            ref (:obj:`Any`):\n",
    "                The PDF reference.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`bool`: :obj:`True` if the reference references\n",
    "            a page, :obj:`False` otherwise.\n",
    "        \"\"\"\n",
    "        return isinstance(ref, dict) and \"Type\" in ref and ref[\"Type\"] is LITERAL_PAGE\n",
    "\n",
    "    def resolve(self, ref: Any) -> Optional[int]:\n",
    "        \"\"\"Resolve a PDF reference to a page number recursively.\n",
    "\n",
    "        Args:\n",
    "            ref (:obj:`Any`):\n",
    "                The PDF reference.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`Optional[int]`: The page number or :obj:`None`\n",
    "            if the reference could not be resolved (e.g., remote Go-To\n",
    "            Actions or malformed references).\n",
    "        \"\"\"\n",
    "        ref_type = self.get_ref_type(ref)\n",
    "\n",
    "        if ref_type is PDFRefType.PDF_OBJ_REF and self.is_ref_page(ref.resolve()):\n",
    "            return self.objid_to_pagenum.get(ref.objid)\n",
    "        elif ref_type is PDFRefType.PDF_OBJ_REF:\n",
    "            return self.resolve(ref.resolve())\n",
    "\n",
    "        if ref_type is PDFRefType.DICTIONARY:\n",
    "            return self.resolve(ref[\"D\"])\n",
    "\n",
    "        if ref_type is PDFRefType.LIST:\n",
    "            # Get the PDFObjRef in the list (usually first element).\n",
    "            return self.resolve(next(filter(lambda e: isinstance(e, PDFObjRef), ref)))\n",
    "\n",
    "        if ref_type is PDFRefType.NAMED_REF:\n",
    "            return self.resolve(self.document.get_dest(ref))\n",
    "\n",
    "        return None  # PDFRefType.UNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8adb355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import shutil\n",
    "from string import digits\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "import traceback\n",
    "import unicodedata\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from collections import defaultdict\n",
    "\n",
    "def remove_control_characters(s):\n",
    "    return \"\".join(ch for ch in s if unicodedata.category(ch)[0]!=\"C\")\n",
    "\n",
    "# a stopwordlist may be used, but we did not do that\n",
    "#stopwordlist=[\"cover page\", \"cover\", \"front cover\", \"back cover\", \"half title\", \"series page\", \"title page\", \"dedication\", \"table of contents\", \"content\", \"contents\", \"acknowledgments\",\"list of figures\", \"list of tables\", \"list of abbreviations\",\"copyright\", \"copyright page\",\"title page\", \"title\", \"imprint and copyright information\", \"credits\", \"references\", \"bibliography\", \"literature\", \"pagina vuota\"]\n",
    "stopwordlist=[]\n",
    "\n",
    "# Open a PDF document.\n",
    "for f in os.listdir(\"../data/PDFs/\"):\n",
    "    \n",
    "    # Checking if the file is a PDF and does not already have a corresponding TOC file\n",
    "    if f.endswith(\".pdf\") and not os.path.exists(\"./TOCs/\"+f[:-4]+\".toc\"):\n",
    "        try:\n",
    "            tocelements=defaultdict(list)\n",
    "            fp = open(str('../data/PDFs/'+f), 'rb')\n",
    "            parser = PDFParser(fp)\n",
    "            document = PDFDocument(parser)\n",
    "            ref_pagenum_resolver = RefPageNumberResolver(document)\n",
    "            print(f)\n",
    "            \n",
    "            # Get the outlines of the document.\n",
    "            try:\n",
    "                outlines = document.get_outlines()\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                outlines=None\n",
    "                \n",
    "            # Writing TOC elements to a file\n",
    "            if outlines:\n",
    "                with open(\"./TOCs/\"+f[:-4]+\".toc\", \"w\", encoding=\"utf-8-sig\") as r:\n",
    "                    \n",
    "                    # Resolving the page number based on different attributes\n",
    "                    for (level,title,dest,a,se) in outlines:\n",
    "                        if dest:\n",
    "                            page_num = ref_pagenum_resolver.resolve(dest)\n",
    "                        elif a:\n",
    "                            page_num = ref_pagenum_resolver.resolve(a)\n",
    "                        elif se:\n",
    "                            page_num = ref_pagenum_resolver.resolve(se)\n",
    "                        else:\n",
    "                            page_num = None\n",
    "                            \n",
    "                        # Cleaning and stripping control characters from the title\n",
    "                        res=remove_control_characters(title).strip('\"').strip(\"\\r\\n\").strip(\"\\n\").strip(\" \")\n",
    "                        if res.lower() not in stopwordlist:\n",
    "                            #print(level, title)\n",
    "                            wr=csv.writer(r)\n",
    "                            title=res\n",
    "                            wr.writerow([level, title, page_num])\n",
    "                            tocelements[level].append(title)\n",
    "                if not tocelements:\n",
    "                    print(\"No TOC in the metadata of file: \"+f)\n",
    "                    os.remove(\"./TOCs/\"+f[:-4]+\".toc\") # Removing the TOC file if no elements were added\n",
    "                    \n",
    "        except Exception:\n",
    "            print(traceback.format_exc())\n",
    "            \n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53b4ea0-f74e-4a0c-ac59-184636919570",
   "metadata": {},
   "source": [
    "## Segmentation with the ground truth GT_TOC data using the XML file representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059fecfb-9efd-40f2-95ec-e04587457e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install lxml\n",
    "#%pip install wordsegment\n",
    "#%pip install --upgrade ipywidgets\n",
    "\n",
    "from collections import defaultdict, OrderedDict\n",
    "import logging\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import string\n",
    "import signal\n",
    "import traceback\n",
    "from lxml import etree\n",
    "from fuzzywuzzy import fuzz\n",
    "from wordsegment import load, segment\n",
    "import tqdm.notebook as tq\n",
    "import chardet\n",
    "import re\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='./log/unmatched_headings_gt.log', level=logging.WARNING, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Initialize wordsegment\n",
    "load()\n",
    "\n",
    "# Custom exception for timeout handling\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "# Custom OrderedDict subclass with default list values\n",
    "class OrderedDictWithDefaultList(OrderedDict):\n",
    "    def __missing__(self, key):\n",
    "        value = []\n",
    "        self[key] = value\n",
    "        return value\n",
    "\n",
    "# Signal handler for timeout\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutException\n",
    "\n",
    "signal.signal(signal.SIGALRM, timeout_handler)\n",
    "\n",
    "# Function to detect encoding of a file\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        detector = chardet.universaldetector.UniversalDetector()\n",
    "        for line in file:\n",
    "            detector.feed(line)\n",
    "            if detector.done:\n",
    "                break\n",
    "        detector.close()\n",
    "    return detector.result['encoding']\n",
    "\n",
    "# Function to parse TOC files and return a defaultdict of lists\n",
    "def parse_toc_files(toc_path, failed_files):\n",
    "    toc = defaultdict(list)\n",
    "    for f in sorted(os.listdir(toc_path)):\n",
    "        if f.endswith(\".toc\") and f\"{f[:-4]}\" not in failed_files:\n",
    "            enc = detect_encoding(os.path.join(toc_path, f))\n",
    "            with open(os.path.join(toc_path, f), \"r\", encoding=enc) as r:\n",
    "                data = csv.reader(r, escapechar='\\\\')\n",
    "                for d in data:\n",
    "                    if len(d) == 3:\n",
    "                        level, heading, page = d\n",
    "                        toc[f[:-4]].append([level, heading, page])\n",
    "    return toc\n",
    "\n",
    "# Function to load XML content from a file path\n",
    "def load_xml(xml_path):\n",
    "    with open(xml_path, 'rb') as f:\n",
    "        xml_content = f.read()\n",
    "    return etree.fromstring(xml_content)\n",
    "\n",
    "from collections import defaultdict\n",
    "from lxml import etree\n",
    "import re\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Function to group <text> elements by their top attribute\n",
    "def group_text_by_top(text_elements):\n",
    "    lines = defaultdict(list)\n",
    "    for txt in text_elements:\n",
    "        top = txt.get(\"top\")\n",
    "        lines[top].append(txt)\n",
    "    return lines\n",
    "\n",
    "# Function to combine text from nodes grouped by top attribute\n",
    "def combine_text_by_top(lines):\n",
    "    combined_texts = {}\n",
    "    for top, texts in lines.items():\n",
    "        combined_texts[top] = ' '.join(\n",
    "            filter(None, [concatenate_text_from_node(txt) for txt in texts if isinstance(txt, etree._Element)])\n",
    "        ).strip()\n",
    "    return combined_texts\n",
    "\n",
    "# Function to concatenate text content from nodes, handling special formatting tags\n",
    "def concatenate_text_from_node(node):\n",
    "    texts = []\n",
    "    for child in node.iter():\n",
    "        if isinstance(child, etree._ElementUnicodeResult):\n",
    "            if child:\n",
    "                texts.append(child.strip())\n",
    "        elif child.tag in ['b', 'i', 'u', 'font', 'a']:  # Adjust as per your XML structure\n",
    "            if child.text:\n",
    "                texts.append(child.text.strip())\n",
    "            if child.tail:\n",
    "                texts.append(child.tail.strip())\n",
    "    return ' '.join(filter(None, texts)).strip()\n",
    "\n",
    "# Function to check exact match between text and heading\n",
    "def matches_heading_exact(text, heading):\n",
    "    if text is None or heading is None:\n",
    "        return False\n",
    "    return text.lower().strip().replace(\" \", \"\") == heading.lower().strip().replace(\" \", \"\")\n",
    "\n",
    "# Function to find substring match using regex between text and heading\n",
    "def matches_heading(text, heading):\n",
    "    if text is None or heading is None:\n",
    "        return False\n",
    "    clean_text = re.sub(r'[^\\w\\s]', '', text.strip()).lower().replace(\" \", \"\") if text else \"\"\n",
    "    clean_heading = re.sub(r'[^\\w\\s]', '', heading.strip()).lower().replace(\" \", \"\") if heading else \"\"\n",
    "  \n",
    "# Function to check fuzzy match between text and heading\n",
    "def fuzzy_match_heading(text, heading):\n",
    "    if text is None or heading is None:\n",
    "        return False\n",
    "    return fuzz.partial_ratio(re.sub(r'[^\\w\\s]', '', text).strip().lower(), re.sub(r'[^\\w\\s]', '', heading).strip().lower()) == 100 or re.sub(r'[^\\w\\s]', '', heading).strip().lower() in re.sub(r'[^\\w\\s]', '', text).strip().lower()\n",
    "\n",
    "\n",
    "\n",
    "def segment_text(xml, toc, file_key):\n",
    "    segments = []\n",
    "    headings_per_page = defaultdict(list)\n",
    "\n",
    "    # Convert toc_entries to a list of tuples with (heading, page_number, level)\n",
    "    toc_entries = [(entry[1], int(entry[2]), int(entry[0])) for entry in toc]\n",
    "\n",
    "    # Add a dummy heading with a maximum page number\n",
    "    max_page_number = max([int(page.get(\"number\")) for page in xml.xpath('.//page')])\n",
    "    dummy_heading = (\"DUMMY_HEADING\", max_page_number + 1, 0)\n",
    "    toc_entries.append(dummy_heading)\n",
    "    \n",
    "    current_section = {}\n",
    "    toc_index = 0\n",
    "    total_entries = len(toc_entries)\n",
    "    \n",
    "    for page in xml.xpath('.//page'):\n",
    "        page_number = int(page.get(\"number\"))\n",
    "        text_elements = page.xpath('.//text')\n",
    "    \n",
    "        # Group text elements by their top attribute\n",
    "        lines = group_text_by_top(text_elements)\n",
    "        combined_texts = combine_text_by_top(lines)\n",
    "    \n",
    "        found_match = False\n",
    "    \n",
    "        # Build a dictionary of headings per page\n",
    "        while toc_index < total_entries and toc_entries[toc_index][1] == page_number:\n",
    "            heading, _, level = toc_entries[toc_index]\n",
    "            headings_per_page[page_number].append((heading, level))\n",
    "            toc_index += 1\n",
    "        for heading, level in headings_per_page[page_number]:\n",
    "            found_match = False\n",
    "        # Iterate over text elements to find matches\n",
    "            for txt in text_elements:\n",
    "                text_content = concatenate_text_from_node(txt)\n",
    "                found_match = False\n",
    "                # Check if we have an entry in toc_entries for this page_number\n",
    "                if page_number in headings_per_page:\n",
    "                \n",
    "                    # Exact match\n",
    "                    if matches_heading_exact(txt.text, heading) or matches_heading_exact(text_content, heading):\n",
    "                        if current_section is not None:\n",
    "                            segments.append(current_section)\n",
    "                        current_section = {'level':level,'heading': heading, 'content':[]}\n",
    "                        found_match = True\n",
    "                        break\n",
    "    \n",
    "                    # Fuzzy match or regex match\n",
    "                    if matches_heading(txt.text, heading) or matches_heading(text_content, heading):\n",
    "                        if current_section is not None:\n",
    "                            segments.append(current_section)\n",
    "                        current_section = {'level':level,'heading': heading, 'content':[]}\n",
    "                        found_match = True\n",
    "                        break\n",
    "    \n",
    "                    if fuzzy_match_heading(txt.text, heading) or fuzzy_match_heading(text_content, heading):\n",
    "                        if current_section is not None:\n",
    "                            segments.append(current_section)\n",
    "                        current_section = {'level':level,'heading': heading, 'content':[]}\n",
    "                        found_match = True\n",
    "                        break\n",
    "    \n",
    "                # Append text content to current section if no match yet\n",
    "                if not found_match:\n",
    "                    if current_section:\n",
    "                        current_section['content'].append(txt.text or text_content)\n",
    "    \n",
    "            # Check for exact, regex, and fuzzy matches for combined text lines\n",
    "            if not found_match:\n",
    "                for combined_text in combined_texts.values():\n",
    "                    found_match = False\n",
    "                    if page_number in headings_per_page:\n",
    "                        \n",
    "                            \n",
    "                            if matches_heading_exact(combined_text, heading):\n",
    "                                if current_section is not None:\n",
    "                                    segments.append(current_section)\n",
    "                                current_section = {'level':level,'heading': heading, 'content':[]}\n",
    "                                found_match = True\n",
    "                                break\n",
    "        \n",
    "                            if matches_heading(combined_text, heading):\n",
    "                                if current_section is not None:\n",
    "                                    segments.append(current_section)\n",
    "                                current_section = {'level':level,'heading': heading, 'content':[]}\n",
    "                                found_match = True\n",
    "                                break\n",
    "        \n",
    "                            if fuzzy_match_heading(combined_text, heading):\n",
    "                                if current_section is not None:\n",
    "                                    segments.append(current_section)\n",
    "                                current_section = {'level':level,'heading': heading, 'content':[]}\n",
    "                                found_match = True\n",
    "                                break\n",
    "        \n",
    "                    # Append combined text to current section if no match yet\n",
    "                    if not found_match:\n",
    "                        current_section['content'].append(combined_text)\n",
    "        \n",
    "        # Log unmatched headings for the current page\n",
    "        if not found_match and page_number in headings_per_page:\n",
    "            for heading, level in headings_per_page[page_number]:\n",
    "                logging.warning(f'Unmatched heading: \"{heading}\" on page {page_number} in file {file_key}')\n",
    "                print(f'Unmatched heading: \"{heading}\" on page {page_number} in file {file_key}')\n",
    "                input()\n",
    "    # Add the last current section if it exists\n",
    "    if current_section is not None:\n",
    "        segments.append(current_section)\n",
    "    return segments\n",
    "\n",
    "# Main function to orchestrate the process\n",
    "def main():\n",
    "    log_path = \"./log/gt_segmentation_log.txt\"\n",
    "    toc_path = \"../data/GT_TOCs/\"\n",
    "    xml_base_path = \"./xml/\"\n",
    "\n",
    "    # Read failed files from log\n",
    "    with open(log_path, \"r\", encoding=\"utf-8-sig\") as log:\n",
    "        failed_files = log.readlines()\n",
    "\n",
    "    # Parse TOC files\n",
    "    toc = parse_toc_files(toc_path, failed_files)\n",
    "\n",
    "    # Iterate over each TOC entry and process corresponding XML file\n",
    "    for k, v in tq.tqdm(toc.items()):\n",
    "        print(v)\n",
    "        xml_file_path = os.path.join(xml_base_path, f\"{k}.xml\")\n",
    "        if os.path.exists(xml_file_path) and k!= \"industrial-concentration-and-the-chicago-school-of-antitrust-analysis\":\n",
    "            enc2 = detect_encoding(xml_file_path)\n",
    "            print(enc2)\n",
    "            print(k)\n",
    "            print(\"exists\")\n",
    "            xml = load_xml(xml_file_path)\n",
    "            try:\n",
    "                signal.alarm(60 * 15)  # Set a timeout of 15 minutes\n",
    "                segments = segment_text(xml, toc[k], k)\n",
    "                signal.alarm(0)  # Reset the alarm\n",
    "\n",
    "                if len(segments) > 1:\n",
    "                    with open(f\"../data/gt_segments/segments_{k}.json\", 'w', encoding=\"utf-8-sig\") as s:\n",
    "                        json.dump(segments, s)\n",
    "                    \n",
    "                else:\n",
    "                    with open(log_path, \"a\", encoding=\"utf-8-sig\") as log:\n",
    "                        log.write(f\"{k}\\n\")\n",
    "            except TimeoutException:\n",
    "                with open(log_path, \"a\", encoding=\"utf-8-sig\") as log:\n",
    "                    log.write(f\"{k}\\n\")\n",
    "            except Exception:\n",
    "                print(traceback.format_exc())\n",
    "                print(\"fail\")\n",
    "                print(k)\n",
    "                #input()\n",
    "                with open(log_path, \"a\", encoding=\"utf-8-sig\") as log:\n",
    "                    log.write(f\"{k}\\n\")\n",
    "        else:\n",
    "            print(\"Does not exist:\")\n",
    "            print(xml_file_path)\n",
    "            #input()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69ecfe2-88cf-4f9e-9788-208f17093cf6",
   "metadata": {},
   "source": [
    "## Getting the TOC-based parser data segmented, as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6699305f-cd14-4504-aec1-7759367b3103",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, OrderedDict\n",
    "import logging\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import string\n",
    "import signal\n",
    "import traceback\n",
    "from lxml import etree\n",
    "from fuzzywuzzy import fuzz\n",
    "from wordsegment import load, segment\n",
    "import tqdm.notebook as tq\n",
    "import chardet\n",
    "import re\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='./log/unmatched_headings_toc.log', level=logging.WARNING, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Initialize wordsegment\n",
    "load()\n",
    "\n",
    "# Custom exception for timeout handling\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "# Custom OrderedDict subclass with default list values\n",
    "class OrderedDictWithDefaultList(OrderedDict):\n",
    "    def __missing__(self, key):\n",
    "        value = []\n",
    "        self[key] = value\n",
    "        return value\n",
    "\n",
    "# Signal handler for timeout\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutException\n",
    "\n",
    "signal.signal(signal.SIGALRM, timeout_handler)\n",
    "\n",
    "# Function to detect encoding of a file\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        detector = chardet.universaldetector.UniversalDetector()\n",
    "        for line in file:\n",
    "            detector.feed(line)\n",
    "            if detector.done:\n",
    "                break\n",
    "        detector.close()\n",
    "    return detector.result['encoding']\n",
    "\n",
    "# Function to parse TOC files and return a defaultdict of lists\n",
    "def parse_toc_files(toc_path, failed_files):\n",
    "    toc = defaultdict(list)\n",
    "    for f in sorted(os.listdir(toc_path)):\n",
    "        if f.endswith(\".toc\") and f\"{f[:-4]}\" not in failed_files and f in os.listdir(\"../data/GT_TOCs/\"):\n",
    "            enc = detect_encoding(os.path.join(toc_path, f))\n",
    "            with open(os.path.join(toc_path, f), \"r\", encoding=enc) as r:\n",
    "                data = csv.reader(r, escapechar='\\\\')\n",
    "                for d in data:\n",
    "                    if len(d) == 3:\n",
    "                        level, heading, page = d\n",
    "                        toc[f[:-4]].append([level, heading, page])\n",
    "    return toc\n",
    "\n",
    "# Function to load XML content from a file path\n",
    "def load_xml(xml_path):\n",
    "    with open(xml_path, 'rb') as f:\n",
    "        xml_content = f.read()\n",
    "    return etree.fromstring(xml_content)\n",
    "\n",
    "from collections import defaultdict\n",
    "from lxml import etree\n",
    "import re\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Function to group <text> elements by their top attribute\n",
    "def group_text_by_top(text_elements):\n",
    "    lines = defaultdict(list)\n",
    "    for txt in text_elements:\n",
    "        top = txt.get(\"top\")\n",
    "        lines[top].append(txt)\n",
    "    return lines\n",
    "\n",
    "# Function to combine text from nodes grouped by top attribute\n",
    "def combine_text_by_top(lines):\n",
    "    combined_texts = {}\n",
    "    for top, texts in lines.items():\n",
    "        combined_texts[top] = ' '.join(\n",
    "            filter(None, [concatenate_text_from_node(txt) for txt in texts if isinstance(txt, etree._Element)])\n",
    "        ).strip()\n",
    "    return combined_texts\n",
    "\n",
    "# Function to concatenate text content from nodes, handling special formatting tags\n",
    "def concatenate_text_from_node(node):\n",
    "    texts = []\n",
    "    for child in node.iter():\n",
    "        if isinstance(child, etree._ElementUnicodeResult):\n",
    "            if child:\n",
    "                texts.append(child.strip())\n",
    "        elif child.tag in ['b', 'i', 'u', 'font', 'a']:  # Adjust as per your XML structure\n",
    "            if child.text:\n",
    "                texts.append(child.text.strip())\n",
    "            if child.tail:\n",
    "                texts.append(child.tail.strip())\n",
    "    return ' '.join(filter(None, texts)).strip()\n",
    "\n",
    "# Function to check exact match between text and heading\n",
    "def matches_heading_exact(text, heading):\n",
    "    if text is None or heading is None:\n",
    "        return False\n",
    "    return text.lower().strip().replace(\" \", \"\") == heading.lower().strip().replace(\" \", \"\")\n",
    "\n",
    "# Function to find substring match using regex between text and heading\n",
    "def matches_heading(text, heading):\n",
    "    if text is None or heading is None:\n",
    "        return False\n",
    "    clean_text = re.sub(r'[^\\w\\s]', '', text.strip()).lower().replace(\" \", \"\") if text else \"\"\n",
    "    clean_heading = re.sub(r'[^\\w\\s]', '', heading.strip()).lower().replace(\" \", \"\") if heading else \"\"\n",
    "  \n",
    "# Function to check fuzzy match between text and heading\n",
    "def fuzzy_match_heading(text, heading):\n",
    "    if text is None or heading is None:\n",
    "        return False\n",
    "    return fuzz.partial_ratio(re.sub(r'[^\\w\\s]', '', text).strip().lower(), re.sub(r'[^\\w\\s]', '', heading).strip().lower()) == 100 or re.sub(r'[^\\w\\s]', '', heading).strip().lower() in re.sub(r'[^\\w\\s]', '', text).strip().lower()\n",
    "\n",
    "\n",
    "\n",
    "def segment_text(xml, toc, file_key):\n",
    "    segments = []\n",
    "    headings_per_page = defaultdict(list)\n",
    "\n",
    "    # Convert toc_entries to a list of tuples with (heading, page_number, level)\n",
    "    toc_entries = [(entry[1], int(entry[2]), int(entry[0])) for entry in toc]\n",
    "\n",
    "    # Add a dummy heading with a maximum page number\n",
    "    max_page_number = max([int(page.get(\"number\")) for page in xml.xpath('.//page')])\n",
    "    dummy_heading = (\"DUMMY_HEADING\", max_page_number + 1, 0)\n",
    "    toc_entries.append(dummy_heading)\n",
    "    \n",
    "    current_section = {}\n",
    "    toc_index = 0\n",
    "    total_entries = len(toc_entries)\n",
    "    \n",
    "    for page in xml.xpath('.//page'):\n",
    "        page_number = int(page.get(\"number\"))\n",
    "        text_elements = page.xpath('.//text')\n",
    "    \n",
    "        # Group text elements by their top attribute\n",
    "        lines = group_text_by_top(text_elements)\n",
    "        combined_texts = combine_text_by_top(lines)\n",
    "    \n",
    "        found_match = False\n",
    "    \n",
    "        # Build a dictionary of headings per page\n",
    "        while toc_index < total_entries and toc_entries[toc_index][1] == page_number:\n",
    "            heading, _, level = toc_entries[toc_index]\n",
    "            headings_per_page[page_number].append((heading, level))\n",
    "            toc_index += 1\n",
    "        for heading, level in headings_per_page[page_number]:\n",
    "            found_match = False\n",
    "        # Iterate over text elements to find matches\n",
    "            for txt in text_elements:\n",
    "                text_content = concatenate_text_from_node(txt)\n",
    "                found_match = False\n",
    "                # Check if we have an entry in toc_entries for this page_number\n",
    "                if page_number in headings_per_page:\n",
    "                \n",
    "                    # Exact match\n",
    "                    if matches_heading_exact(txt.text, heading) or matches_heading_exact(text_content, heading):\n",
    "                        if current_section is not None:\n",
    "                            segments.append(current_section)\n",
    "                        current_section = {'level':level,'heading': heading, 'content':[]}\n",
    "                        found_match = True\n",
    "                        break\n",
    "    \n",
    "                    # Fuzzy match or regex match\n",
    "                    if matches_heading(txt.text, heading) or matches_heading(text_content, heading):\n",
    "                        if current_section is not None:\n",
    "                            segments.append(current_section)\n",
    "                        current_section = {'level':level,'heading': heading, 'content':[]}\n",
    "                        found_match = True\n",
    "                        break\n",
    "    \n",
    "                    if fuzzy_match_heading(txt.text, heading) or fuzzy_match_heading(text_content, heading):\n",
    "                        if current_section is not None:\n",
    "                            segments.append(current_section)\n",
    "                        current_section = {'level':level,'heading': heading, 'content':[]}\n",
    "                        found_match = True\n",
    "                        break\n",
    "    \n",
    "                # Append text content to current section if no match yet\n",
    "                if not found_match:\n",
    "                    if current_section:\n",
    "                        current_section['content'].append(txt.text or text_content)\n",
    "    \n",
    "            # Check for exact, regex, and fuzzy matches for combined text lines\n",
    "            if not found_match:\n",
    "                for combined_text in combined_texts.values():\n",
    "                    found_match = False\n",
    "                    if page_number in headings_per_page:\n",
    "                        \n",
    "                            \n",
    "                            if matches_heading_exact(combined_text, heading):\n",
    "                                if current_section is not None:\n",
    "                                    segments.append(current_section)\n",
    "                                current_section = {'level':level,'heading': heading, 'content':[]}\n",
    "                                found_match = True\n",
    "                                break\n",
    "        \n",
    "                            if matches_heading(combined_text, heading):\n",
    "                                if current_section is not None:\n",
    "                                    segments.append(current_section)\n",
    "                                current_section = {'level':level,'heading': heading, 'content':[]}\n",
    "                                found_match = True\n",
    "                                break\n",
    "        \n",
    "                            if fuzzy_match_heading(combined_text, heading):\n",
    "                                if current_section is not None:\n",
    "                                    segments.append(current_section)\n",
    "                                current_section = {'level':level,'heading': heading, 'content':[]}\n",
    "                                found_match = True\n",
    "                                break\n",
    "        \n",
    "                    # Append combined text to current section if no match yet\n",
    "                    if not found_match:\n",
    "                        if current_section:\n",
    "                            current_section['content'].append(combined_text)\n",
    "        \n",
    "        # Log unmatched headings for the current page\n",
    "        if not found_match and page_number in headings_per_page:\n",
    "            for heading, level in headings_per_page[page_number]:\n",
    "                logging.warning(f'Unmatched heading: \"{heading}\" on page {page_number} in file {file_key}')\n",
    "                print(f'Unmatched heading: \"{heading}\" on page {page_number} in file {file_key}')\n",
    "                #input()\n",
    "    # Add the last current section if it exists\n",
    "    if current_section is not None:\n",
    "        segments.append(current_section)\n",
    "    return segments\n",
    "\n",
    "# Main function to orchestrate the process\n",
    "def main():\n",
    "    log_path = \"./log/segmentation_log.txt\"\n",
    "    toc_path = \"./TOCs/\"\n",
    "    xml_base_path = \"./xml/\"\n",
    "\n",
    "    # Read failed files from log\n",
    "    with open(log_path, \"r\", encoding=\"utf-8-sig\") as log:\n",
    "        failed_files = log.readlines()\n",
    "\n",
    "    # Parse TOC files\n",
    "    toc = parse_toc_files(toc_path, failed_files)\n",
    "\n",
    "    # Iterate over each TOC entry and process corresponding XML file\n",
    "    for k, v in tq.tqdm(toc.items()):\n",
    "        print(k)\n",
    "        xml_file_path = os.path.join(xml_base_path, f\"{k}.xml\")\n",
    "        if os.path.exists(xml_file_path) and k!= \"industrial-concentration-and-the-chicago-school-of-antitrust-analysis\":\n",
    "            enc2 = detect_encoding(xml_file_path)\n",
    "            print(enc2)\n",
    "            #print(k)\n",
    "            print(\"exists\")\n",
    "            xml = load_xml(xml_file_path)\n",
    "            try:\n",
    "                signal.alarm(60 * 15)  # Set a timeout of 15 minutes\n",
    "                segments = segment_text(xml, toc[k], k)\n",
    "                signal.alarm(0)  # Reset the alarm\n",
    "\n",
    "                if len(segments) > 1:\n",
    "                    with open(f\"./segments/segments_{k}.json\", 'w', encoding=\"utf-8-sig\") as s:\n",
    "                        json.dump(segments, s)\n",
    "                else:\n",
    "                    print(\"seglength\")\n",
    "                    print(segments)\n",
    "                    input()\n",
    "                    with open(log_path, \"a\", encoding=\"utf-8-sig\") as log:\n",
    "                        log.write(f\"{k}\\n\")\n",
    "            except TimeoutException:\n",
    "                with open(log_path, \"a\", encoding=\"utf-8-sig\") as log:\n",
    "                    log.write(f\"{k}\\n\")\n",
    "            except Exception:\n",
    "                print(traceback.format_exc())\n",
    "                \n",
    "                with open(log_path, \"a\", encoding=\"utf-8-sig\") as log:\n",
    "                    log.write(f\"{k}\\n\")\n",
    "        else:\n",
    "            print(\"Does not exist:\")\n",
    "            print(xml_file_path)\n",
    "            \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e71ddf4",
   "metadata": {},
   "source": [
    "## Finally segmenting also the LLM-Refined PageParser Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91775e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6-traditional-medicines-law-and-the-dis-ordering-of-temporalities.toc\n",
      "a-bird-that-flies-with-two-wings-kastom-and-state-justice-systems-in-vanuatu.toc\n",
      "a-kind-of-mending-restorative-justice-in-the-pacific-islands.toc\n",
      "access-controlled-the-shaping-of-power-rights-and-rule-in-cyberspace.toc\n",
      "access-to-justice-and-legal-empowerment-making-the-poor-central-in-legal-development-co-operati.toc\n",
      "access-to-knowledge-in-the-age-of-intellectual-property.toc\n",
      "accountability-and-the-law-rights-authority-and-transparency-of-public-power.toc\n",
      "administrative-decision-making-in-australian-migration-l.toc\n",
      "advancing-equality-how-constitutional-rights-can-make-a-difference-worldwide.toc\n",
      "aegis-or-achilles-heel-the-dilemma-of-homology-in-biopatents-in-the-wake-of-novozymes.toc\n",
      "agriculture-and-food-security-in-china-what-effect-wto-accession-and-regional-trade-arrangements.toc\n",
      "al-haq-a-global-history-of-the-first-palestinian-human-rights-organizati.toc\n",
      "amicus-curiae-before-international-courts-and-tribunals.toc\n",
      "analogy-and-exemplary-reasoning-in-legal-discourse.toc\n",
      "antitrust-enforcement-and-standard-essential-patents-moving-beyond-the-frand-commitment.toc\n",
      "applicable-law-in-investor-state-arbitrati.toc\n",
      "applying-shari-a-in-the-west-facts-fears-and-the-future-of-islamic-rules-on-family-relations-in-the-west.toc\n",
      "australian-clinical-legal-education-designing-and-operating-a-best-practice-clinical-program-in-an-australian-law-sch.toc\n",
      "bank-regulation-risk-management-and-compliance-theory-practice-and-key-problem-areas.toc\n",
      "basic-income-tax-2016-2017.toc\n",
      "bioethics-and-the-patent-eligibility-of-human-embryonic-stem-cells-related-inventions-in-europe.toc\n",
      "boats-to-burn-bajo-fishing-activity-in-the-australian-fishing-zone.toc\n",
      "border-flows-a-century-of-the-canadian-american-water-relationshi.toc\n",
      "brexit-and-the-future-of-eu-politics-a-constitutional-law-perspective.toc\n",
      "climate-change-and-international-shipping-the-regulatory-framework-for-the-reduction-of-greenhouse-gas-emissions.toc\n",
      "consumer-data-protection-in-brazil-china-and-germany-a-comparative-study.toc\n",
      "employee-participation-and-collective-bargaining-in-europe-and-chi.toc\n",
      "german-and-asian-perspectives-on-company-law-law-and-policy-perspectives.toc\n",
      "industrial-concentration-and-the-chicago-school-of-antitrust-analysis.toc\n",
      "intellectual-property-in-the-global-arena-jurisdiction-applicable-law-and-the-recognition-of-judgments-in-europe-japan-and-the-us.toc\n",
      "introduction-to-swiss-l.toc\n",
      "necessity-or-nuisance-recourse-to-human-rights-in-substantive-international-criminal-l.toc\n",
      "patents-and-public-health-legalising-the-policy-thoughts-in-the-doha-trips-declaration-of-14-november-2001.toc\n",
      "populist-challenges-to-constitutional-interpretation-in-europe-and-bey.toc\n",
      "reconsidering-constitutional-formation-i-national-sovereignty-a-comparative-analysis-of-the-juridification-by-constituti.toc\n",
      "refining-child-pornography-law-crime-language-and-social-consequences.toc\n",
      "revisiting-chinas-competition-law-and-its-interaction-with-intellectual-property-rights.toc\n",
      "second-generation-patents-in-pharmaceutical-innovati.toc\n",
      "the-constitution-and-governance-in-camer.toc\n",
      "the-endangered-species-act-history-implementation-successes-and-controversies.toc\n",
      "the-future-of-the-law-of-the-sea-bridging-gaps-between-national-individual-and-common-interests.toc\n",
      "the-law-applicable-to-security-interests-in-intermediated-securities-under-ohada-l.toc\n",
      "the-principle-of-purpose-limitation-in-data-protection-laws-the-risk-based-approach-principles-and-private-standards-as-elements-for-regulating-innovati.toc\n",
      "the-regulation-of-megabanks-legal-frameworks-of-the-usa-and-eu.toc\n",
      "the-role-of-the-patent-system-in-stimulating-innovation-and-technology-transfer-for-climate-change-including-aspects-of-licensing-and-competition-l.toc\n",
      "u-s-federal-income-taxation-of-individuals-2016.toc\n",
      "un-human-rights-mechanisms-and-the-environment-synergies-challenges-trajectories.toc\n",
      "women-and-the-un-a-new-history-of-womens-international-human-rights.toc\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff85e037f6646ff94da6ebdc5823e9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "titles_6-traditional-medicines-law-and-the-dis-ordering-of-temporalities\n",
      "xml:\n",
      "./xml/6-traditional-medicines-law-and-the-dis-ordering-of-temporalities.xml\n",
      "utf-8\n",
      "exists\n",
      "titles_a-bird-that-flies-with-two-wings-kastom-and-state-justice-systems-in-vanuatu\n",
      "xml:\n",
      "./xml/a-bird-that-flies-with-two-wings-kastom-and-state-justice-systems-in-vanuatu.xml\n",
      "utf-8\n",
      "exists\n",
      "titles_a-kind-of-mending-restorative-justice-in-the-pacific-islands\n",
      "xml:\n",
      "./xml/a-kind-of-mending-restorative-justice-in-the-pacific-islands.xml\n",
      "utf-8\n",
      "exists\n",
      "Unmatched heading: \"Introduction\" on page 59 in file titles_a-kind-of-mending-restorative-justice-in-the-pacific-islands\n",
      "Unmatched heading: \"tribal warfare and transformative justice in the new guinea highlands\" on page 59 in file titles_a-kind-of-mending-restorative-justice-in-the-pacific-islands\n",
      "Unmatched heading: \"Alan Rumsey\" on page 59 in file titles_a-kind-of-mending-restorative-justice-in-the-pacific-islands\n",
      "Unmatched heading: \"Department of Anthropology\" on page 59 in file titles_a-kind-of-mending-restorative-justice-in-the-pacific-islands\n",
      "Unmatched heading: \"Research School of Pacific and Asian Studies\" on page 59 in file titles_a-kind-of-mending-restorative-justice-in-the-pacific-islands\n",
      "Unmatched heading: \"Australian National University\" on page 59 in file titles_a-kind-of-mending-restorative-justice-in-the-pacific-islands\n",
      "titles_access-controlled-the-shaping-of-power-rights-and-rule-in-cyberspace\n",
      "xml:\n",
      "./xml/access-controlled-the-shaping-of-power-rights-and-rule-in-cyberspace.xml\n",
      "utf-8\n",
      "exists\n",
      "titles_access-to-justice-and-legal-empowerment-making-the-poor-central-in-legal-development-co-operati\n",
      "xml:\n",
      "./xml/access-to-justice-and-legal-empowerment-making-the-poor-central-in-legal-development-co-operati.xml\n",
      "utf-8\n",
      "exists\n",
      "Unmatched heading: \"Law, Governance and Development\" on page 1 in file titles_access-to-justice-and-legal-empowerment-making-the-poor-central-in-legal-development-co-operati\n",
      "Unmatched heading: \"Research & Policy Notes\" on page 1 in file titles_access-to-justice-and-legal-empowerment-making-the-poor-central-in-legal-development-co-operati\n",
      "Unmatched heading: \"Access to Justice and Legal Empowerment\" on page 1 in file titles_access-to-justice-and-legal-empowerment-making-the-poor-central-in-legal-development-co-operati\n",
      "Unmatched heading: \"Making the Poor Central in Legal Development Co-operation\" on page 1 in file titles_access-to-justice-and-legal-empowerment-making-the-poor-central-in-legal-development-co-operati\n",
      "Unmatched heading: \"Ineke van de Meene and Benjamin van Rooij\" on page 1 in file titles_access-to-justice-and-legal-empowerment-making-the-poor-central-in-legal-development-co-operati\n",
      "titles_access-to-knowledge-in-the-age-of-intellectual-property\n",
      "xml:\n",
      "./xml/access-to-knowledge-in-the-age-of-intellectual-property.xml\n",
      "utf-8\n",
      "exists\n",
      "titles_accountability-and-the-law-rights-authority-and-transparency-of-public-power\n",
      "xml:\n",
      "./xml/accountability-and-the-law-rights-authority-and-transparency-of-public-power.xml\n",
      "utf-8\n",
      "exists\n",
      "titles_administrative-decision-making-in-australian-migration-l\n",
      "xml:\n",
      "./xml/administrative-decision-making-in-australian-migration-l.xml\n",
      "utf-8\n",
      "exists\n",
      "titles_advancing-equality-how-constitutional-rights-can-make-a-difference-worldwide\n",
      "xml:\n",
      "./xml/advancing-equality-how-constitutional-rights-can-make-a-difference-worldwide.xml\n",
      "utf-8\n",
      "exists\n",
      "titles_aegis-or-achilles-heel-the-dilemma-of-homology-in-biopatents-in-the-wake-of-novozymes\n",
      "xml:\n",
      "./xml/aegis-or-achilles-heel-the-dilemma-of-homology-in-biopatents-in-the-wake-of-novozymes.xml\n",
      "utf-8\n",
      "exists\n",
      "titles_agriculture-and-food-security-in-china-what-effect-wto-accession-and-regional-trade-arrangements\n",
      "xml:\n",
      "./xml/agriculture-and-food-security-in-china-what-effect-wto-accession-and-regional-trade-arrangements.xml\n",
      "utf-8\n",
      "exists\n",
      "Unmatched heading: \"Agricultural trade between China and ASEAN: complementary or competitive?\" on page 39 in file titles_agriculture-and-food-security-in-china-what-effect-wto-accession-and-regional-trade-arrangements\n",
      "Unmatched heading: \"Legal and legislative changes\" on page 64 in file titles_agriculture-and-food-security-in-china-what-effect-wto-accession-and-regional-trade-arrangements\n",
      "Unmatched heading: \"Agricultural trade reforms\" on page 66 in file titles_agriculture-and-food-security-in-china-what-effect-wto-accession-and-regional-trade-arrangements\n",
      "Unmatched heading: \"Domestic market reform and infrastructure development\" on page 67 in file titles_agriculture-and-food-security-in-china-what-effect-wto-accession-and-regional-trade-arrangements\n",
      "Unmatched heading: \"Land-use policy\" on page 68 in file titles_agriculture-and-food-security-in-china-what-effect-wto-accession-and-regional-trade-arrangements\n",
      "Unmatched heading: \"Farm organisations\" on page 69 in file titles_agriculture-and-food-security-in-china-what-effect-wto-accession-and-regional-trade-arrangements\n",
      "Unmatched heading: \"Export subsidies and agricultural support policies\" on page 69 in file titles_agriculture-and-food-security-in-china-what-effect-wto-accession-and-regional-trade-arrangements\n",
      "Unmatched heading: \"New agricultural and rural development policies\" on page 70 in file titles_agriculture-and-food-security-in-china-what-effect-wto-accession-and-regional-trade-arrangements\n",
      "Unmatched heading: \"Reduction of market distortion measures\" on page 77 in file titles_agriculture-and-food-security-in-china-what-effect-wto-accession-and-regional-trade-arrangements\n",
      "Unmatched heading: \"Growth and institutional reform\" on page 81 in file titles_agriculture-and-food-security-in-china-what-effect-wto-accession-and-regional-trade-arrangements\n",
      "Unmatched heading: \"Trade pattern and comparative advantage in China’s agriculture\" on page 83 in file titles_agriculture-and-food-security-in-china-what-effect-wto-accession-and-regional-trade-arrangements\n",
      "Unmatched heading: \"The setting for agricultural liberalisation\" on page 84 in file titles_agriculture-and-food-security-in-china-what-effect-wto-accession-and-regional-trade-arrangements\n",
      "Unmatched heading: \"Adjustment costs of WTO entry and their income implications\" on page 88 in file titles_agriculture-and-food-security-in-china-what-effect-wto-accession-and-regional-trade-arrangements\n",
      "titles_al-haq-a-global-history-of-the-first-palestinian-human-rights-organizati\n",
      "xml:\n",
      "./xml/al-haq-a-global-history-of-the-first-palestinian-human-rights-organizati.xml\n",
      "utf-8\n",
      "exists\n",
      "titles_amicus-curiae-before-international-courts-and-tribunals\n",
      "xml:\n",
      "./xml/amicus-curiae-before-international-courts-and-tribunals.xml\n",
      "utf-8\n",
      "exists\n",
      "titles_analogy-and-exemplary-reasoning-in-legal-discourse\n",
      "xml:\n",
      "./xml/analogy-and-exemplary-reasoning-in-legal-discourse.xml\n",
      "utf-8\n",
      "exists\n",
      "titles_antitrust-enforcement-and-standard-essential-patents-moving-beyond-the-frand-commitment\n",
      "xml:\n",
      "./xml/antitrust-enforcement-and-standard-essential-patents-moving-beyond-the-frand-commitment.xml\n",
      "utf-8\n",
      "exists\n",
      "titles_applicable-law-in-investor-state-arbitrati\n",
      "xml:\n",
      "./xml/applicable-law-in-investor-state-arbitrati.xml\n",
      "utf-8\n",
      "exists\n",
      "Unmatched heading: \"3. The Method and Plan of the Study\" on page 45 in file titles_applicable-law-in-investor-state-arbitrati\n",
      "Unmatched heading: \"Chapter 3\" on page 45 in file titles_applicable-law-in-investor-state-arbitrati\n",
      "Unmatched heading: \"Chapter 4\" on page 45 in file titles_applicable-law-in-investor-state-arbitrati\n",
      "Unmatched heading: \"Chapter 5\" on page 45 in file titles_applicable-law-in-investor-state-arbitrati\n",
      "titles_applying-shari-a-in-the-west-facts-fears-and-the-future-of-islamic-rules-on-family-relations-in-the-west\n",
      "xml:\n",
      "./xml/applying-shari-a-in-the-west-facts-fears-and-the-future-of-islamic-rules-on-family-relations-in-the-west.xml\n",
      "utf-8\n",
      "exists\n",
      "titles_australian-clinical-legal-education-designing-and-operating-a-best-practice-clinical-program-in-an-australian-law-sch\n",
      "xml:\n",
      "./xml/australian-clinical-legal-education-designing-and-operating-a-best-practice-clinical-program-in-an-australian-law-sch.xml\n",
      "utf-8\n",
      "exists\n",
      "Unmatched heading: \"What is reflection?\" on page 168 in file titles_australian-clinical-legal-education-designing-and-operating-a-best-practice-clinical-program-in-an-australian-law-sch\n",
      "Unmatched heading: \"Why do we want to teach reflective practice?\" on page 172 in file titles_australian-clinical-legal-education-designing-and-operating-a-best-practice-clinical-program-in-an-australian-law-sch\n",
      "Unmatched heading: \"Reflection aids the educative process, while experience aids reflection\" on page 172 in file titles_australian-clinical-legal-education-designing-and-operating-a-best-practice-clinical-program-in-an-australian-law-sch\n",
      "Unmatched heading: \"Reflection to expose students to law in context\" on page 178 in file titles_australian-clinical-legal-education-designing-and-operating-a-best-practice-clinical-program-in-an-australian-law-sch\n",
      "titles_bank-regulation-risk-management-and-compliance-theory-practice-and-key-problem-areas\n",
      "xml:\n",
      "./xml/bank-regulation-risk-management-and-compliance-theory-practice-and-key-problem-areas.xml\n",
      "utf-8\n",
      "exists\n",
      "Unmatched heading: \"Theory, Practice, and Key Problem Areas\" on page 4 in file titles_bank-regulation-risk-management-and-compliance-theory-practice-and-key-problem-areas\n",
      "Unmatched heading: \"1.3 The rise of the global financial conglomerate and BHC structure\" on page 67 in file titles_bank-regulation-risk-management-and-compliance-theory-practice-and-key-problem-areas\n",
      "titles_basic-income-tax-2016-2017\n",
      "xml:\n",
      "./xml/basic-income-tax-2016-2017.xml\n",
      "utf-8\n",
      "exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "titles_bioethics-and-the-patent-eligibility-of-human-embryonic-stem-cells-related-inventions-in-europe\n",
      "xml:\n",
      "./xml/bioethics-and-the-patent-eligibility-of-human-embryonic-stem-cells-related-inventions-in-europe.xml\n",
      "utf-8\n",
      "exists\n",
      "Unmatched heading: \"Harvard Onco-mouse\" on page 44 in file titles_bioethics-and-the-patent-eligibility-of-human-embryonic-stem-cells-related-inventions-in-europe\n",
      "Unmatched heading: \"T 19/90\" on page 44 in file titles_bioethics-and-the-patent-eligibility-of-human-embryonic-stem-cells-related-inventions-in-europe\n",
      "Unmatched heading: \"vis-u00e0-vis\" on page 44 in file titles_bioethics-and-the-patent-eligibility-of-human-embryonic-stem-cells-related-inventions-in-europe\n",
      "titles_boats-to-burn-bajo-fishing-activity-in-the-australian-fishing-zone\n",
      "xml:\n",
      "./xml/boats-to-burn-bajo-fishing-activity-in-the-australian-fishing-zone.xml\n",
      "utf-8\n",
      "exists\n",
      "Unmatched heading: \"Published by ANU E Press\" on page 4 in file titles_boats-to-burn-bajo-fishing-activity-in-the-australian-fishing-zone\n",
      "Unmatched heading: \"The Australian National University\" on page 4 in file titles_boats-to-burn-bajo-fishing-activity-in-the-australian-fishing-zone\n",
      "Unmatched heading: \"Canberra ACT 0200, Australia\" on page 4 in file titles_boats-to-burn-bajo-fishing-activity-in-the-australian-fishing-zone\n",
      "Unmatched heading: \"Email: anuepress@anu.edu.au\" on page 4 in file titles_boats-to-burn-bajo-fishing-activity-in-the-australian-fishing-zone\n",
      "Unmatched heading: \"This title is also available online at: http://epress.anu.edu.au/boats_citation.html\" on page 4 in file titles_boats-to-burn-bajo-fishing-activity-in-the-australian-fishing-zone\n",
      "Unmatched heading: \"National Library of Australia\" on page 4 in file titles_boats-to-burn-bajo-fishing-activity-in-the-australian-fishing-zone\n",
      "Unmatched heading: \"Cataloguing-in-Publication entry\" on page 4 in file titles_boats-to-burn-bajo-fishing-activity-in-the-australian-fishing-zone\n",
      "Unmatched heading: \"Stacey, Natasha.\" on page 4 in file titles_boats-to-burn-bajo-fishing-activity-in-the-australian-fishing-zone\n",
      "Unmatched heading: \"Boats to burn: Bajo fishing activity in the Australian fishing zone.\" on page 4 in file titles_boats-to-burn-bajo-fishing-activity-in-the-australian-fishing-zone\n",
      "Unmatched heading: \"Bibliography.\" on page 4 in file titles_boats-to-burn-bajo-fishing-activity-in-the-australian-fishing-zone\n",
      "Unmatched heading: \"ISBN 9781920942946 (pbk.)\" on page 4 in file titles_boats-to-burn-bajo-fishing-activity-in-the-australian-fishing-zone\n",
      "Unmatched heading: \"ISBN 9781920942953 (online)\" on page 4 in file titles_boats-to-burn-bajo-fishing-activity-in-the-australian-fishing-zone\n",
      "Unmatched heading: \"305.8992\" on page 4 in file titles_boats-to-burn-bajo-fishing-activity-in-the-australian-fishing-zone\n",
      "Unmatched heading: \"All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying or otherwise, without the prior permission of the publisher.\" on page 4 in file titles_boats-to-burn-bajo-fishing-activity-in-the-australian-fishing-zone\n",
      "Unmatched heading: \"Cover design by Duncan Beard.\" on page 4 in file titles_boats-to-burn-bajo-fishing-activity-in-the-australian-fishing-zone\n",
      "Unmatched heading: \"Cover photographs: Natasha Stacey.\" on page 4 in file titles_boats-to-burn-bajo-fishing-activity-in-the-australian-fishing-zone\n",
      "Unmatched heading: \"This edition u00a9 2007 ANU E Press\" on page 4 in file titles_boats-to-burn-bajo-fishing-activity-in-the-australian-fishing-zone\n",
      "titles_brexit-and-the-future-of-eu-politics-a-constitutional-law-perspective\n",
      "xml:\n",
      "./xml/brexit-and-the-future-of-eu-politics-a-constitutional-law-perspective.xml\n",
      "utf-8\n",
      "exists\n",
      "titles_climate-change-and-international-shipping-the-regulatory-framework-for-the-reduction-of-greenhouse-gas-emissions\n",
      "xml:\n",
      "./xml/climate-change-and-international-shipping-the-regulatory-framework-for-the-reduction-of-greenhouse-gas-emissions.xml\n",
      "utf-8\n",
      "exists\n",
      "titles_consumer-data-protection-in-brazil-china-and-germany-a-comparative-study\n",
      "xml:\n",
      "./xml/consumer-data-protection-in-brazil-china-and-germany-a-comparative-study.xml\n",
      "utf-8\n",
      "exists\n",
      "titles_employee-participation-and-collective-bargaining-in-europe-and-chi\n",
      "xml:\n",
      "./xml/employee-participation-and-collective-bargaining-in-europe-and-chi.xml\n",
      "utf-8\n",
      "exists\n",
      "titles_german-and-asian-perspectives-on-company-law-law-and-policy-perspectives\n",
      "xml:\n",
      "./xml/german-and-asian-perspectives-on-company-law-law-and-policy-perspectives.xml\n",
      "utf-8\n",
      "exists\n",
      "titles_industrial-concentration-and-the-chicago-school-of-antitrust-analysis\n",
      "xml:\n",
      "./xml/industrial-concentration-and-the-chicago-school-of-antitrust-analysis.xml\n",
      "utf-8\n",
      "exists\n",
      "titles_intellectual-property-in-the-global-arena-jurisdiction-applicable-law-and-the-recognition-of-judgments-in-europe-japan-and-the-us\n",
      "xml:\n",
      "./xml/intellectual-property-in-the-global-arena-jurisdiction-applicable-law-and-the-recognition-of-judgments-in-europe-japan-and-the-us.xml\n",
      "utf-8\n",
      "exists\n",
      "titles_introduction-to-swiss-l\n",
      "xml:\n",
      "./xml/introduction-to-swiss-l.xml\n",
      "utf-8\n",
      "exists\n",
      "titles_necessity-or-nuisance-recourse-to-human-rights-in-substantive-international-criminal-l\n",
      "xml:\n",
      "./xml/necessity-or-nuisance-recourse-to-human-rights-in-substantive-international-criminal-l.xml\n",
      "utf-8\n",
      "exists\n",
      "titles_patents-and-public-health-legalising-the-policy-thoughts-in-the-doha-trips-declaration-of-14-november-2001\n",
      "xml:\n",
      "./xml/patents-and-public-health-legalising-the-policy-thoughts-in-the-doha-trips-declaration-of-14-november-2001.xml\n",
      "utf-8\n",
      "exists\n",
      "titles_populist-challenges-to-constitutional-interpretation-in-europe-and-bey\n",
      "xml:\n",
      "./xml/populist-challenges-to-constitutional-interpretation-in-europe-and-bey.xml\n",
      "utf-8\n",
      "exists\n",
      "Unmatched heading: \"Contents\" on page 6 in file titles_populist-challenges-to-constitutional-interpretation-in-europe-and-bey\n",
      "Unmatched heading: \"PART I: Theoretical implications\" on page 6 in file titles_populist-challenges-to-constitutional-interpretation-in-europe-and-bey\n",
      "Unmatched heading: \"Populism and populist constitutionalism\" on page 6 in file titles_populist-challenges-to-constitutional-interpretation-in-europe-and-bey\n",
      "Unmatched heading: \"The art of constitutional interpretation\" on page 6 in file titles_populist-challenges-to-constitutional-interpretation-in-europe-and-bey\n",
      "Unmatched heading: \"An ‘Instrument of Government’ or ‘Instrument of Courts’? The impact of political systems on constitutional interpretation and the case of populism\" on page 6 in file titles_populist-challenges-to-constitutional-interpretation-in-europe-and-bey\n",
      "Unmatched heading: \"Popular initiatives, populism and the Croatian constitutional court\" on page 8 in file titles_populist-challenges-to-constitutional-interpretation-in-europe-and-bey\n",
      "Unmatched heading: \"Constitutional identity as a populist notion? The Council of State and the forging of the Greek constitutional identity through the crisis\" on page 8 in file titles_populist-challenges-to-constitutional-interpretation-in-europe-and-bey\n",
      "Unmatched heading: \"Constitutional interpretation under the new Fundamental Law of Hungary\" on page 8 in file titles_populist-challenges-to-constitutional-interpretation-in-europe-and-bey\n",
      "Unmatched heading: \"The populist reforms in Italy and the instrument of the constitutionally conforming interpretation\" on page 8 in file titles_populist-challenges-to-constitutional-interpretation-in-europe-and-bey\n",
      "Unmatched heading: \"PART III: An Outlook\" on page 10 in file titles_populist-challenges-to-constitutional-interpretation-in-europe-and-bey\n",
      "Unmatched heading: \"15 Born populist: the Trump administration, the courts and the Constitution of the United States\" on page 10 in file titles_populist-challenges-to-constitutional-interpretation-in-europe-and-bey\n",
      "Unmatched heading: \"16 Constitutional interpretation: What can Europeans learn from US debates?\" on page 10 in file titles_populist-challenges-to-constitutional-interpretation-in-europe-and-bey\n",
      "Unmatched heading: \"17 Populist and non-democratic reading of the constitution – Sad lessons from Latin America\" on page 10 in file titles_populist-challenges-to-constitutional-interpretation-in-europe-and-bey\n",
      "titles_reconsidering-constitutional-formation-i-national-sovereignty-a-comparative-analysis-of-the-juridification-by-constituti\n",
      "xml:\n",
      "./xml/reconsidering-constitutional-formation-i-national-sovereignty-a-comparative-analysis-of-the-juridification-by-constituti.xml\n",
      "utf-8\n",
      "exists\n",
      "Unmatched heading: \"3.4.2 The Procedural Openness of May Constitution as Reflex onto the Juridification of National Sovereignty\" on page 47 in file titles_reconsidering-constitutional-formation-i-national-sovereignty-a-comparative-analysis-of-the-juridification-by-constituti\n",
      "titles_refining-child-pornography-law-crime-language-and-social-consequences\n",
      "xml:\n",
      "./xml/refining-child-pornography-law-crime-language-and-social-consequences.xml\n",
      "utf-8\n",
      "exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "titles_revisiting-chinas-competition-law-and-its-interaction-with-intellectual-property-rights\n",
      "xml:\n",
      "./xml/revisiting-chinas-competition-law-and-its-interaction-with-intellectual-property-rights.xml\n",
      "utf-8\n",
      "exists\n",
      "titles_second-generation-patents-in-pharmaceutical-innovati\n",
      "xml:\n",
      "./xml/second-generation-patents-in-pharmaceutical-innovati.xml\n",
      "utf-8\n",
      "exists\n",
      "titles_the-constitution-and-governance-in-camer\n",
      "xml:\n",
      "./xml/the-constitution-and-governance-in-camer.xml\n",
      "utf-8\n",
      "exists\n",
      "titles_the-endangered-species-act-history-implementation-successes-and-controversies\n",
      "xml:\n",
      "./xml/the-endangered-species-act-history-implementation-successes-and-controversies.xml\n",
      "utf-8\n",
      "exists\n",
      "Unmatched heading: \"Roots of Endangered Species Conservation\" on page 8 in file titles_the-endangered-species-act-history-implementation-successes-and-controversies\n",
      "Unmatched heading: \"Some Basic Concepts\" on page 8 in file titles_the-endangered-species-act-history-implementation-successes-and-controversies\n",
      "Unmatched heading: \"The Endangered Species Act: The Statute and the Regulations\" on page 8 in file titles_the-endangered-species-act-history-implementation-successes-and-controversies\n",
      "Unmatched heading: \"Take Permits and Mitigation\" on page 10 in file titles_the-endangered-species-act-history-implementation-successes-and-controversies\n",
      "Unmatched heading: \"The Endangered Species Act and the States\" on page 10 in file titles_the-endangered-species-act-history-implementation-successes-and-controversies\n",
      "Unmatched heading: \"Future of the Endangered Species Act\" on page 10 in file titles_the-endangered-species-act-history-implementation-successes-and-controversies\n",
      "Unmatched heading: \"6.3.2 Habitat Conservation Plans\" on page 170 in file titles_the-endangered-species-act-history-implementation-successes-and-controversies\n",
      "Unmatched heading: \"Regional versus Individual Habitat Conservation Plans\" on page 170 in file titles_the-endangered-species-act-history-implementation-successes-and-controversies\n",
      "Unmatched heading: \"Safe Harbor Agreements\" on page 172 in file titles_the-endangered-species-act-history-implementation-successes-and-controversies\n",
      "Unmatched heading: \"NEPA Compliance for Habitat Conservation Plans\" on page 172 in file titles_the-endangered-species-act-history-implementation-successes-and-controversies\n",
      "titles_the-future-of-the-law-of-the-sea-bridging-gaps-between-national-individual-and-common-interests\n",
      "xml:\n",
      "./xml/the-future-of-the-law-of-the-sea-bridging-gaps-between-national-individual-and-common-interests.xml\n",
      "utf-8\n",
      "exists\n",
      "titles_the-law-applicable-to-security-interests-in-intermediated-securities-under-ohada-l\n",
      "xml:\n",
      "./xml/the-law-applicable-to-security-interests-in-intermediated-securities-under-ohada-l.xml\n",
      "utf-8\n",
      "exists\n",
      "titles_the-principle-of-purpose-limitation-in-data-protection-laws-the-risk-based-approach-principles-and-private-standards-as-elements-for-regulating-innovati\n",
      "xml:\n",
      "./xml/the-principle-of-purpose-limitation-in-data-protection-laws-the-risk-based-approach-principles-and-private-standards-as-elements-for-regulating-innovati.xml\n",
      "utf-8\n",
      "exists\n",
      "titles_the-regulation-of-megabanks-legal-frameworks-of-the-usa-and-eu\n",
      "xml:\n",
      "./xml/the-regulation-of-megabanks-legal-frameworks-of-the-usa-and-eu.xml\n",
      "utf-8\n",
      "exists\n",
      "Unmatched heading: \"Attempts at business model classification\" on page 56 in file titles_the-regulation-of-megabanks-legal-frameworks-of-the-usa-and-eu\n",
      "Unmatched heading: \"Green revolution\" on page 62 in file titles_the-regulation-of-megabanks-legal-frameworks-of-the-usa-and-eu\n",
      "Unmatched heading: \"3.3.2 Regional level\" on page 143 in file titles_the-regulation-of-megabanks-legal-frameworks-of-the-usa-and-eu\n",
      "Unmatched heading: \"3.4 G-SIB leverage ratio\" on page 145 in file titles_the-regulation-of-megabanks-legal-frameworks-of-the-usa-and-eu\n",
      "Unmatched heading: \"3.4.1 International level\" on page 145 in file titles_the-regulation-of-megabanks-legal-frameworks-of-the-usa-and-eu\n",
      "titles_the-role-of-the-patent-system-in-stimulating-innovation-and-technology-transfer-for-climate-change-including-aspects-of-licensing-and-competition-l\n",
      "xml:\n",
      "./xml/the-role-of-the-patent-system-in-stimulating-innovation-and-technology-transfer-for-climate-change-including-aspects-of-licensing-and-competition-l.xml\n",
      "utf-8\n",
      "exists\n",
      "titles_u-s-federal-income-taxation-of-individuals-2016\n",
      "xml:\n",
      "./xml/u-s-federal-income-taxation-of-individuals-2016.xml\n",
      "utf-8\n",
      "exists\n",
      "titles_un-human-rights-mechanisms-and-the-environment-synergies-challenges-trajectories\n",
      "xml:\n",
      "./xml/un-human-rights-mechanisms-and-the-environment-synergies-challenges-trajectories.xml\n",
      "utf-8\n",
      "exists\n",
      "Unmatched heading: \"Proposed General Comment on Environment and Climate Change\" on page 256 in file titles_un-human-rights-mechanisms-and-the-environment-synergies-challenges-trajectories\n",
      "titles_women-and-the-un-a-new-history-of-womens-international-human-rights\n",
      "xml:\n",
      "./xml/women-and-the-un-a-new-history-of-womens-international-human-rights.xml\n",
      "utf-8\n",
      "exists\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, OrderedDict\n",
    "import logging\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import string\n",
    "import signal\n",
    "import traceback\n",
    "from lxml import etree\n",
    "from fuzzywuzzy import fuzz\n",
    "from wordsegment import load, segment\n",
    "import tqdm.notebook as tq\n",
    "import chardet\n",
    "import re\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='./log/llm_unmatched_headings_toc.log', level=logging.WARNING, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Initialize wordsegment\n",
    "load()\n",
    "\n",
    "# Custom exception for timeout handling\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "# Custom OrderedDict subclass with default list values\n",
    "class OrderedDictWithDefaultList(OrderedDict):\n",
    "    def __missing__(self, key):\n",
    "        value = []\n",
    "        self[key] = value\n",
    "        return value\n",
    "\n",
    "# Signal handler for timeout\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutException\n",
    "\n",
    "signal.signal(signal.SIGALRM, timeout_handler)\n",
    "\n",
    "# Function to detect encoding of a file\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        detector = chardet.universaldetector.UniversalDetector()\n",
    "        for line in file:\n",
    "            detector.feed(line)\n",
    "            if detector.done:\n",
    "                break\n",
    "        detector.close()\n",
    "    return detector.result['encoding']\n",
    "\n",
    "# Function to parse TOC files and return a defaultdict of lists\n",
    "def parse_toc_files(toc_path, failed_files):\n",
    "    toc = defaultdict(list)\n",
    "    for f in sorted(os.listdir(toc_path)):\n",
    "        print(str(f[7:-4]+\".toc\"))\n",
    "        if f.endswith(\".csv\") and f\"{f[:-4]}\" not in failed_files and str(f[7:-4]+\".toc\") in os.listdir(\"../data/GT_TOCs/\"):\n",
    "            enc = detect_encoding(os.path.join(toc_path, f))\n",
    "            with open(os.path.join(toc_path, f), \"r\", encoding=enc) as r:\n",
    "                data = csv.reader(r, escapechar='\\\\')\n",
    "                for d in data:\n",
    "                    if len(d) == 3:\n",
    "                        level, heading, page = d\n",
    "                        toc[f[:-4]].append([level, heading, page])\n",
    "    return toc\n",
    "\n",
    "# Function to load XML content from a file path\n",
    "def load_xml(xml_path):\n",
    "    with open(xml_path, 'rb') as f:\n",
    "        xml_content = f.read()\n",
    "    return etree.fromstring(xml_content)\n",
    "\n",
    "from collections import defaultdict\n",
    "from lxml import etree\n",
    "import re\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Function to group <text> elements by their top attribute\n",
    "def group_text_by_top(text_elements):\n",
    "    lines = defaultdict(list)\n",
    "    for txt in text_elements:\n",
    "        top = txt.get(\"top\")\n",
    "        lines[top].append(txt)\n",
    "    return lines\n",
    "\n",
    "# Function to combine text from nodes grouped by top attribute\n",
    "def combine_text_by_top(lines):\n",
    "    combined_texts = {}\n",
    "    for top, texts in lines.items():\n",
    "        combined_texts[top] = ' '.join(\n",
    "            filter(None, [concatenate_text_from_node(txt) for txt in texts if isinstance(txt, etree._Element)])\n",
    "        ).strip()\n",
    "    return combined_texts\n",
    "\n",
    "# Function to concatenate text content from nodes, handling special formatting tags\n",
    "def concatenate_text_from_node(node):\n",
    "    texts = []\n",
    "    for child in node.iter():\n",
    "        if isinstance(child, etree._ElementUnicodeResult):\n",
    "            if child:\n",
    "                texts.append(child.strip())\n",
    "        elif child.tag in ['b', 'i', 'u', 'font', 'a']:  # Adjust as per your XML structure\n",
    "            if child.text:\n",
    "                texts.append(child.text.strip())\n",
    "            if child.tail:\n",
    "                texts.append(child.tail.strip())\n",
    "    return ' '.join(filter(None, texts)).strip()\n",
    "\n",
    "# Function to check exact match between text and heading\n",
    "def matches_heading_exact(text, heading):\n",
    "    if text is None or heading is None:\n",
    "        return False\n",
    "    return text.lower().strip().replace(\" \", \"\") == heading.lower().strip().replace(\" \", \"\")\n",
    "\n",
    "# Function to find substring match using regex between text and heading\n",
    "def matches_heading(text, heading):\n",
    "    if text is None or heading is None:\n",
    "        return False\n",
    "    clean_text = re.sub(r'[^\\w\\s]', '', text.strip()).lower().replace(\" \", \"\") if text else \"\"\n",
    "    clean_heading = re.sub(r'[^\\w\\s]', '', heading.strip()).lower().replace(\" \", \"\") if heading else \"\"\n",
    "  \n",
    "# Function to check fuzzy match between text and heading\n",
    "def fuzzy_match_heading(text, heading):\n",
    "    if text is None or heading is None:\n",
    "        return False\n",
    "    return fuzz.partial_ratio(re.sub(r'[^\\w\\s]', '', text).strip().lower(), re.sub(r'[^\\w\\s]', '', heading).strip().lower()) == 100 or re.sub(r'[^\\w\\s]', '', heading).strip().lower() in re.sub(r'[^\\w\\s]', '', text).strip().lower()\n",
    "\n",
    "\n",
    "\n",
    "def segment_text(xml, toc, file_key):\n",
    "    segments = []\n",
    "    headings_per_page = defaultdict(list)\n",
    "\n",
    "    # Convert toc_entries to a list of tuples with (heading, page_number, level)\n",
    "    toc_entries = [(entry[1], int(entry[2]), int(entry[0])) for entry in toc]\n",
    "\n",
    "    # Add a dummy heading with a maximum page number\n",
    "    max_page_number = max([int(page.get(\"number\")) for page in xml.xpath('.//page')])\n",
    "    dummy_heading = (\"DUMMY_HEADING\", max_page_number + 1, 0)\n",
    "    toc_entries.append(dummy_heading)\n",
    "    \n",
    "    current_section = {}\n",
    "    toc_index = 0\n",
    "    total_entries = len(toc_entries)\n",
    "    \n",
    "    for page in xml.xpath('.//page'):\n",
    "        page_number = int(page.get(\"number\"))\n",
    "        text_elements = page.xpath('.//text')\n",
    "    \n",
    "        # Group text elements by their top attribute\n",
    "        lines = group_text_by_top(text_elements)\n",
    "        combined_texts = combine_text_by_top(lines)\n",
    "    \n",
    "        found_match = False\n",
    "    \n",
    "        # Build a dictionary of headings per page\n",
    "        while toc_index < total_entries and toc_entries[toc_index][1] == page_number:\n",
    "            heading, _, level = toc_entries[toc_index]\n",
    "            headings_per_page[page_number].append((heading, level))\n",
    "            toc_index += 1\n",
    "        for heading, level in headings_per_page[page_number]:\n",
    "            found_match = False\n",
    "        # Iterate over text elements to find matches\n",
    "            for txt in text_elements:\n",
    "                text_content = concatenate_text_from_node(txt)\n",
    "                found_match = False\n",
    "                # Check if we have an entry in toc_entries for this page_number\n",
    "                if page_number in headings_per_page:\n",
    "                \n",
    "                    # Exact match\n",
    "                    if matches_heading_exact(txt.text, heading) or matches_heading_exact(text_content, heading):\n",
    "                        if current_section is not None:\n",
    "                            segments.append(current_section)\n",
    "                        current_section = {'level':level,'heading': heading, 'content':[]}\n",
    "                        found_match = True\n",
    "                        break\n",
    "    \n",
    "                    # Fuzzy match or regex match\n",
    "                    if matches_heading(txt.text, heading) or matches_heading(text_content, heading):\n",
    "                        if current_section is not None:\n",
    "                            segments.append(current_section)\n",
    "                        current_section = {'level':level,'heading': heading, 'content':[]}\n",
    "                        found_match = True\n",
    "                        break\n",
    "    \n",
    "                    if fuzzy_match_heading(txt.text, heading) or fuzzy_match_heading(text_content, heading):\n",
    "                        if current_section is not None:\n",
    "                            segments.append(current_section)\n",
    "                        current_section = {'level':level,'heading': heading, 'content':[]}\n",
    "                        found_match = True\n",
    "                        break\n",
    "    \n",
    "                # Append text content to current section if no match yet\n",
    "                if not found_match:\n",
    "                    if current_section:\n",
    "                        current_section['content'].append(txt.text or text_content)\n",
    "    \n",
    "            # Check for exact, regex, and fuzzy matches for combined text lines\n",
    "            if not found_match:\n",
    "                for combined_text in combined_texts.values():\n",
    "                    found_match = False\n",
    "                    if page_number in headings_per_page:\n",
    "                        \n",
    "                            \n",
    "                            if matches_heading_exact(combined_text, heading):\n",
    "                                if current_section is not None:\n",
    "                                    segments.append(current_section)\n",
    "                                current_section = {'level':level,'heading': heading, 'content':[]}\n",
    "                                found_match = True\n",
    "                                break\n",
    "        \n",
    "                            if matches_heading(combined_text, heading):\n",
    "                                if current_section is not None:\n",
    "                                    segments.append(current_section)\n",
    "                                current_section = {'level':level,'heading': heading, 'content':[]}\n",
    "                                found_match = True\n",
    "                                break\n",
    "        \n",
    "                            if fuzzy_match_heading(combined_text, heading):\n",
    "                                if current_section is not None:\n",
    "                                    segments.append(current_section)\n",
    "                                current_section = {'level':level,'heading': heading, 'content':[]}\n",
    "                                found_match = True\n",
    "                                break\n",
    "        \n",
    "                    # Append combined text to current section if no match yet\n",
    "                    if not found_match:\n",
    "                        if current_section:\n",
    "                            current_section['content'].append(combined_text)\n",
    "        \n",
    "        # Log unmatched headings for the current page\n",
    "        if not found_match and page_number in headings_per_page:\n",
    "            for heading, level in headings_per_page[page_number]:\n",
    "                logging.warning(f'Unmatched heading: \"{heading}\" on page {page_number} in file {file_key}')\n",
    "                print(f'Unmatched heading: \"{heading}\" on page {page_number} in file {file_key}')\n",
    "                #input()\n",
    "    # Add the last current section if it exists\n",
    "    if current_section is not None:\n",
    "        segments.append(current_section)\n",
    "    return segments\n",
    "\n",
    "# Main function to orchestrate the process\n",
    "def main():\n",
    "    log_path = \"./log/llm_segmentation_log.txt\"\n",
    "    toc_path = \"../title_candidates_clean/\"\n",
    "    xml_base_path = \"./xml/\"\n",
    "\n",
    "    # Read failed files from log\n",
    "    with open(log_path, \"r\", encoding=\"utf-8-sig\") as log:\n",
    "        failed_files = log.readlines()\n",
    "\n",
    "    # Parse TOC files\n",
    "    toc = parse_toc_files(toc_path, failed_files)\n",
    "\n",
    "    # Iterate over each TOC entry and process corresponding XML file\n",
    "    for k, v in tq.tqdm(toc.items()):\n",
    "        print(k)\n",
    "        xml_file_path = os.path.join(xml_base_path, f\"{k[7:]}.xml\")\n",
    "        print(\"xml:\")\n",
    "        print(xml_file_path)\n",
    "        if os.path.exists(xml_file_path) and k!= \"industrial-concentration-and-the-chicago-school-of-antitrust-analysis\":\n",
    "            enc2 = detect_encoding(xml_file_path)\n",
    "            print(enc2)\n",
    "            #print(k)\n",
    "            print(\"exists\")\n",
    "            xml = load_xml(xml_file_path)\n",
    "            try:\n",
    "                signal.alarm(60 * 15)  # Set a timeout of 15 minutes\n",
    "                segments = segment_text(xml, toc[k], k)\n",
    "                signal.alarm(0)  # Reset the alarm\n",
    "\n",
    "                if len(segments) > 1:\n",
    "                    with open(f\"./llm_segments/segments_{k[7:]}.json\", 'w', encoding=\"utf-8-sig\") as s:\n",
    "                        json.dump(segments, s)\n",
    "                else:\n",
    "                    print(\"seglength\")\n",
    "                    print(segments)\n",
    "                    input()\n",
    "                    with open(log_path, \"a\", encoding=\"utf-8-sig\") as log:\n",
    "                        log.write(f\"{k}\\n\")\n",
    "            except TimeoutException:\n",
    "                with open(log_path, \"a\", encoding=\"utf-8-sig\") as log:\n",
    "                    log.write(f\"{k}\\n\")\n",
    "            except Exception:\n",
    "                print(traceback.format_exc())\n",
    "                \n",
    "                with open(log_path, \"a\", encoding=\"utf-8-sig\") as log:\n",
    "                    log.write(f\"{k}\\n\")\n",
    "        else:\n",
    "            print(\"Does not exist:\")\n",
    "            print(xml_file_path)\n",
    "            \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ee0d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
